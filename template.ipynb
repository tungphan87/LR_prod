{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from copy import copy, deepcopy\n",
    "import logging\n",
    "import configparser\n",
    "from sqlalchemy import MetaData, create_engine\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "pd.set_option('display.max_columns', None)\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pgdb\n",
    "import s3fs\n",
    "import boto3\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date as dt\n",
    "from datetime import datetime, timedelta\n",
    "import datetime\n",
    "from LR_prod import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query_Redshit: \n",
    "    def __init__(self,dbname): \n",
    "        self.dbname = dbname\n",
    "        self.conn = None\n",
    "\n",
    "    def _get_credentials(self): \n",
    "        database = self.dbname\n",
    "\n",
    "        if database == 'vdp':\n",
    "        # ingress = {\n",
    "        #     'securityGroupId': 'sg-1e2e7460',  # redshift security group\n",
    "        #     'port': 8192,  # redshift port\n",
    "        #     'region': 'us-west-2',  # redshift region\n",
    "        #     'materialSet': 'com.amazon.access.Kindle-Planning-Prod-EiderScience-1'  # iam user able to add ingress rule\n",
    "        # }\n",
    "        # eider.network.authorizeIngress(**ingress)\n",
    "        # vdp_credentials = eider.odin.getCredential(\"com.amazon.kindle-planning-redshift.ro.prod.eider\")\n",
    "            connection = {\n",
    "                'host': 'kindle-planning-redshift.cfbl7dhwcatl.us-west-2.redshift.amazonaws.com',\n",
    "                'dbname': 'prod',\n",
    "                'user': 'kindle_planning_ro', \n",
    "                'password': 'cYsYP6Mhd#VFec%6TrBA',\n",
    "                'port':8192\n",
    "            }\n",
    "        elif database == 'dmap':\n",
    "            # ingress = {\n",
    "            #     'securityGroupId': 'sg-0154da99c0fd65b92',  # redshift security group\n",
    "            #     'port': 8192,  # redshift port\n",
    "            #     'region': 'us-east-1',  # redshift region\n",
    "            #     'materialSet': 'com.amazon.access.Dial-DMaP-Data-Prod-EiderScienceTeam-1'  # iam user able to add ingress rule\n",
    "            # }\n",
    "            # eider.network.authorizeIngress(**ingress)\n",
    "            # credentials = eider.odin.getCredential(\"com.amazon.dmap.data.redshift.prod.ro_direct\")\n",
    "            connection = {\n",
    "                'host': 'dmap-data-cluster-prod.cbdo75nipkyk.us-east-1.redshift.amazonaws.com',\n",
    "                'dbname': 'dmapdataclusterprod',\n",
    "                'user':'dmap_data_prod_rw',\n",
    "                'password': '%TGY2+)1BU2daHs' ,\n",
    "                'port':8192\n",
    "            }\n",
    "            # conn = pgdb.connect(**dmap_connection)\n",
    "            # eider.network.revokeIngress(**ingress)\n",
    "        # Analytics Cluster    \n",
    "        elif database == 'analytics':\n",
    "            # ingress = {\n",
    "            #     'securityGroupId': 'sg-0a8c44f0f1f6936f9',  # redshift security group\n",
    "            #     'port': 8192,  # redshift port\n",
    "            #     'region': 'us-east-1',  # redshift region\n",
    "            #     'materialSet': 'com.amazon.access.Kindle-Planning-Corp-EiderDmap-1'  # iam user able to add ingress rule\n",
    "            # }\n",
    "            # eider.network.authorizeIngress(**ingress)\n",
    "            # analytic_credentials = eider.odin.getCredential(\"com.amazon.kindle-planning-redshift.device-planning-analytics.dial_dmap_ro\")\n",
    "            connection = {\n",
    "                'host': 'device-planning-analytics.clszfooyvile.us-east-1.redshift.amazonaws.com',\n",
    "                'dbname': 'deviceplanning',\n",
    "                'user': 'dial_dmap_ro', \n",
    "                'password': 'RO85711f12260729afbfdfaf2ddaeb139f',\n",
    "                'port':8192\n",
    "            }\n",
    "            # conn = pgdb.connect(**analytic_connection)\n",
    "            # eider.network.revokeIngress(**ingress)\n",
    "        # DMAP Beta Cluster    \n",
    "        elif database == 'dmap_beta':\n",
    "            # ingress = {\n",
    "            #     'securityGroupId': 'sg-072276cf843295057',\n",
    "            #     'port': 8192,\n",
    "            #     'region': 'us-east-1',\n",
    "            #     'materialSet': 'com.amazon.access.Dial-DMaP-Data-Beta-S3user-1'\n",
    "            # }\n",
    "            # eider.network.authorizeIngress(**ingress)\n",
    "            \n",
    "            # credentials = eider.odin.getCredential(\"com.amazon.dmap.data.redshift.beta.dmap_beta_rw\")\n",
    "            connection = {\n",
    "                'host': 'dmap-data-cluster-beta.c9aj4xzdfqrn.us-east-1.redshift.amazonaws.com',\n",
    "                'dbname': 'dmapdataclusterbeta',\n",
    "                'user':'eider_rw',\n",
    "                'password': '3NCPSnM06smf9Q0mFw7yRSwhwccOYyflvGJPgXaM',\n",
    "                'port':8192\n",
    "            }\n",
    "            # conn = pgdb.connect(**dmap_connection)\n",
    "            # eider.network.revokeIngress(**ingress)\n",
    "        # VDP Science Prod Cluster\n",
    "        elif database == 'vdp_science':\n",
    "            # ingress = {\n",
    "            #     'securityGroupId': 'sg-056373d3b84cc25ae',\n",
    "            #     'port': 8192,\n",
    "            #     'region': 'us-east-1',\n",
    "            #     'materialSet': 'com.amazon.access.Dial-Vdp-Science-Data-Catalog-Prod-Eider-VDP-User-1'\n",
    "            # }\n",
    "            # eider.network.authorizeIngress(**ingress)\n",
    "            \n",
    "            # credentials = eider.odin.getCredential(\"com.amazon.dial-vdp-science.vdp-redshift.vdpscience_rs_prod\")\n",
    "            connection = {\n",
    "                'host': 'vdpscienceprod.ceuabgn11vdw.us-east-1.redshift.amazonaws.com',\n",
    "                'dbname': 'vdpscienceprod',\n",
    "                'user': 'vdpscience_rs_prod',\n",
    "                'password': 'B9ZscC3y$WSXvxb',\n",
    "                'port':8192\n",
    "            }\n",
    "            # conn = pgdb.connect(**vdp_connection)\n",
    "        # eider.network.revokeIngress(**ingress)\n",
    "   \n",
    "        else:\n",
    "            print(\"Please choose database from ('vdp','dmap','analytics','dmap_beta','vdp_science')\")\n",
    "            return\n",
    "        self.credentials = connection\n",
    "\n",
    "\n",
    "    def connect(self): \n",
    "        # print(\"connecting {}\".format(self.dbname))\n",
    "        self._get_credentials()\n",
    "        self.conn = pgdb.connect(**self.credentials)\n",
    "\n",
    "    def query(self, query_string): \n",
    "        # print(query_string)\n",
    "        if self.conn is None: # connect when self.conn is None\n",
    "            self.connect()\n",
    "        df = pd.read_sql(sql = query_string, con = self.conn)\n",
    "        self.conn.close()\n",
    "        return df \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_marketplace(country):\n",
    "    marketplace_mapping = {'US': ['1', '1338980', '1071830', '1259700'],\n",
    "    'UK': ['3', '330551'],\n",
    "    'DE': ['4', '330871'],\n",
    "    'CA': ['7', '1398340'],\n",
    "    'JP': ['6', '121322'],\n",
    "    'IN': ['44571'],\n",
    "    'AU': ['111172'],\n",
    "    'FR': ['5', '330921'],\n",
    "    'IT': ['35691', '330711'],\n",
    "    'ES': ['44551', '330731'],\n",
    "    'MX': ['771770'],\n",
    "    'BR': ['526970'], \n",
    "    'CN': ['NA']}\n",
    "    marketplace_id = marketplace_mapping[country]\n",
    "    marketplace_id = ','.join([\"'\"+x+\"'\" for x in marketplace_id])\n",
    "    marketplace_id = '(' + marketplace_id + ')'\n",
    "    return marketplace_id\n",
    "\n",
    "def query_actuals(country, channel, device_type, start_date, end_date):\n",
    "\n",
    "    query = \"\"\"\n",
    "    select date, country, channel, device_type, program, a.asin\n",
    "    , sum(actual) as actual\n",
    "    from attainments_cache a join asins using(asin)\n",
    "    where country = '{country}'\n",
    "    and channel = '{channel}'\n",
    "    and device_type = '{device_type}'\n",
    "    and date between '{start_date}' and '{end_date}'\n",
    "    and program not in ('Doppler','Fox') \n",
    "    group by 1,2,3,4,5,6\n",
    "    order by 1,2,3,4,5,6\"\"\".format(country = country, channel = channel, device_type = device_type,\n",
    "                                  start_date = start_date, end_date = end_date)\n",
    "    vdp_db = Query_Redshit('vdp')\n",
    "    df = vdp_db.query(query)\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "\n",
    "    return df\n",
    "\n",
    "def query_vdps(country, channel, device_type, start_date, end_date, fct_version):\n",
    "\n",
    "    query = \"\"\"select date, from_country as country, channel, device_type, program, asin, sum(vdp_plus_units) as vdp_plus_units\n",
    "    from vdp_retailer_vdps\n",
    "    where from_country = '{country}'\n",
    "    and channel = '{channel}'\n",
    "    and device_type = '{device_type}'\n",
    "    and date between '{start_date}' and '{end_date}'\n",
    "    and fct_version = '{fct_version}'\n",
    "    and program not in ('Doppler','Fox') \n",
    "    group by 1,2,3,4,5,6\"\"\".format(country = country, channel = channel, device_type = device_type\n",
    "                                , start_date = start_date, end_date = end_date, fct_version = fct_version)\n",
    "    df = query_redshift(query)\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def query_asin_end(country, channel, device_type):\n",
    "    query = \"\"\"\n",
    "    select program, asin, min(date) as asin_start_date, max(date) as asin_end_date\n",
    "    from ATTAINMENTS_CACHE join asins using(asin)\n",
    "    where 1=1\n",
    "    and country = '{country}'\n",
    "    and channel = '{channel}'\n",
    "    and device_type = '{device_type}'\n",
    "    and baseline_units > 0\n",
    "    group by 1,2\n",
    "    order by 1\"\"\".format(country = country, channel = channel, device_type = device_type)\n",
    "    vdp_science_db = Query_Redshit('vdp')\n",
    "    df = vdp_science_db.query(query)\n",
    "    df.asin_end_date = pd.to_datetime(df.asin_end_date)\n",
    "    df.asin_start_date = pd.to_datetime(df.asin_start_date)\n",
    "    return df\n",
    "\n",
    "def query_asp(country, channel, device_type, start_date, end_date):\n",
    "\n",
    "    query = \"\"\"\n",
    "    select order_day as date, country, channel, a.device_type, program, a.asin\n",
    "    , alp\n",
    "    , asp\n",
    "    , ordered_qty as qty\n",
    "    from adept.price_elasticity_by_saket_asin a join adept.asins using(asin)\n",
    "    where country = '{country}'\n",
    "    and a.channel = '{channel}'\n",
    "    and a.device_type = '{device_type}'\n",
    "    and a.order_day between '{start_date}' and '{end_date}'\"\"\".format(country = country, channel = channel\n",
    "    , device_type = device_type, start_date = start_date, end_date = end_date)\n",
    "    vdp_science_db = Query_Redshit('vdp_science')\n",
    "    df = vdp_science_db.query(query)\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "\n",
    "    return df\n",
    "\n",
    "def query_vlt(country, channel , device_type, start_date, end_date):\n",
    "    \"\"\"\n",
    "        query vlt\n",
    "    \"\"\"\n",
    "    market_place_id = get_marketplace(country)\n",
    "    query = \"\"\"\n",
    "    select  date(snapshot_day) date, asin, avg(vlt) as vlt\n",
    "    from dialvdpfeaturecatalog.d_daily_asin_vlt \n",
    "    where marketplace_id in {market_place_id}\n",
    "    group by 1,2\n",
    "    order by date\"\"\".format(market_place_id = market_place_id)\n",
    "    vdp_science_prod_db = Query_Redshit('vdp_science')\n",
    "    df = vdp_science_prod_db.query(query)\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "    return df\n",
    "\n",
    "def query_asin_attr(country, channel, device_type, start_date, end_date):\n",
    "    \"\"\"\n",
    "        query asin attributes\n",
    "    \"\"\"\n",
    "    query = \"select *  from dialvdpfeaturecatalog.d_device_asins \\\n",
    "    where country = '{country}' \\\n",
    "    and device_type = '{device_type}'\".format(country = country, device_type = device_type)\n",
    "\n",
    "    vdp_science_prod_db = Query_Redshit('vdp_science')\n",
    "    df = vdp_science_prod_db.query(query)\n",
    "    if 'date' in df.columns:\n",
    "        df.date = pd.to_datetime(df.date)\n",
    "    return df\n",
    "\n",
    "def query_holidays(country, channel, device_type, holiday_name, start_year, end_year):\n",
    "\n",
    "    query = \"\"\"\n",
    "    select date(calendar_date)\n",
    "    from dialvdpfeaturecatalog.d_holiday_attributes \n",
    "    WHERE  country = '{country}' \n",
    "           AND calendar_year BETWEEN '{start_year}' AND '{end_year}' \n",
    "           AND holiday = '{holiday_name}'\n",
    "    \"\"\".format(country=country,\n",
    "                start_year=start_year,\n",
    "                end_year=end_year,\n",
    "                holiday_name=holiday_name)\n",
    "    vdp_science_prod_db = Query_Redshit('vdp_science')\n",
    "    df = vdp_science_prod_db.query(query)\n",
    "    dates = df.date.tolist()\n",
    "    return dates\n",
    "\n",
    "def generate_holidays(country, channel, device_type, start_date, end_date):\n",
    "\n",
    "\n",
    "    index = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    df_holiday_binary = pd.DataFrame(index=index, columns=['date'])\n",
    "    df_holiday_binary.date = df_holiday_binary.index\n",
    "    start_year = pd.to_datetime(start_date).year \n",
    "    end_year = pd.to_datetime(end_date).year\n",
    "\n",
    "    dates_Prime = query_holidays(country,channel,device_type,'Prime Day',start_year,end_year)\n",
    "    dates_Prime += [dt(2018, 7, 16)]\n",
    "\n",
    "\n",
    "    dates_Prime_m1w = [date for sublist in\n",
    "                           [pd.date_range(x - timedelta(7), x - timedelta(2)).tolist()\n",
    "                            for x in dates_Prime]\n",
    "                           for date in sublist]\n",
    "    dates_Prime_m2w = [date for sublist in\n",
    "                       [pd.date_range(x - timedelta(14), x - timedelta(8)).tolist()\n",
    "                        for x in dates_Prime]\n",
    "                       for date in sublist]\n",
    "    dates_Prime_p1w = [date for sublist in\n",
    "                       [pd.date_range(x + timedelta(2), x + timedelta(7)).tolist()\n",
    "                        for x in dates_Prime]\n",
    "                       for date in sublist]\n",
    "    dates_Prime_m3w = [date for sublist in\n",
    "                       [pd.date_range(x - timedelta(21), x - timedelta(15)).tolist()\n",
    "                        for x in dates_Prime]\n",
    "                       for date in sublist]\n",
    "\n",
    "    dates_Prime_p2w = [date for sublist in\n",
    "                       [pd.date_range(x + timedelta(8), x + timedelta(14)).tolist()\n",
    "                        for x in dates_Prime]\n",
    "                       for date in sublist]\n",
    "\n",
    "    dates_Prime_p3w = [date for sublist in\n",
    "                       [pd.date_range(x + timedelta(15), x + timedelta(21)).tolist()\n",
    "                        for x in dates_Prime]\n",
    "                       for date in sublist]\n",
    "    df_holiday_binary.loc[dates_Prime, 'PD'] = 1\n",
    "    df_holiday_binary.loc[dates_Prime_m1w, 'PD_m1w'] = 1\n",
    "    df_holiday_binary.loc[dates_Prime_p1w, 'PD_p1w'] = 1\n",
    "    df_holiday_binary.loc[dates_Prime_p2w, 'PD_p2w'] = 1\n",
    "    df_holiday_binary.loc[dates_Prime_m1w, 'PD_m1w'] = 1\n",
    "    df_holiday_binary.loc[dates_Prime_p1w, 'PD_p1w'] = 1\n",
    "    \n",
    "    if country in ['US','AU','CA']:\n",
    "        dates_Alexa = query_holidays(country,channel,device_type,'Alexa\\'\\'s Birthday',start_year,end_year)\n",
    "        dates_Alexa_m1w = [date for sublist in\n",
    "                               [pd.date_range(x - timedelta(7), x - timedelta(1)).tolist()\n",
    "                                for x in dates_Alexa]\n",
    "                               for date in sublist]\n",
    "        dates_Alexa_p1w = [date for sublist in\n",
    "                           [pd.date_range(x + timedelta(1), x + timedelta(7)).tolist()\n",
    "                            for x in dates_Alexa]\n",
    "                           for date in sublist]\n",
    "        df_holiday_binary.loc[dates_Alexa, 'Alexa'] = 1\n",
    "        df_holiday_binary.loc[dates_Alexa_m1w, 'Alexa_m1w'] = 1\n",
    "        df_holiday_binary.loc[dates_Alexa_p1w, 'Alexa_p1w'] = 1\n",
    "    if country == 'UK':\n",
    "        dates_bank_summar = query_holidays(country,channel,device_type,'Summer Bank Holiday',start_year, end_year)\n",
    "        dates_bank_spring = query_holidays(country, channel, device_type, 'Spring Bank Holiday', start_year, end_year)\n",
    "        dates_bank_summar_m1w = [date for sublist in\n",
    "                               [pd.date_range(x - timedelta(7), x - timedelta(1)).tolist()\n",
    "                                for x in dates_bank_summar]\n",
    "                               for date in sublist]\n",
    "        dates_bank_summar_p1w = [date for sublist in\n",
    "                           [pd.date_range(x + timedelta(1), x + timedelta(7)).tolist()\n",
    "                            for x in dates_bank_summar]\n",
    "                           for date in sublist]\n",
    "        dates_bank_spring_m1w = [date for sublist in\n",
    "                               [pd.date_range(x - timedelta(7), x - timedelta(1)).tolist()\n",
    "                                for x in dates_bank_spring]\n",
    "                               for date in sublist]\n",
    "        dates_bank_spring_p1w = [date for sublist in\n",
    "                           [pd.date_range(x + timedelta(1), x + timedelta(7)).tolist()\n",
    "                            for x in dates_bank_spring]\n",
    "                           for date in sublist]\n",
    "        df_holiday_binary.loc[dates_bank_summar, 'summar_bank'] = 1\n",
    "        df_holiday_binary.loc[dates_bank_spring, 'spring_bank'] = 1\n",
    "        df_holiday_binary.loc[dates_bank_summar_m1w, 'summar_bank_m1w'] = 1\n",
    "        df_holiday_binary.loc[dates_bank_spring_m1w, 'spring_bank_m1w'] = 1\n",
    "        df_holiday_binary.loc[dates_bank_summar_p1w, 'summar_bank_p1w'] = 1\n",
    "        df_holiday_binary.loc[dates_bank_spring_p1w, 'spring_bank_p1w'] = 1\n",
    "        \n",
    "    if country in ['US', 'CA', 'MX', 'UK', 'DE', 'FR', 'IT', 'ES', 'JP', 'BR', 'AU']:\n",
    "        \n",
    "        # remove easter for now \n",
    "        # dates_eastern = query_holidays(country, channel, device_type,'Easter Monday',start_year,end_year)\n",
    "        # dates_eastern_m1w = [date for sublist in\n",
    "        #                        [pd.date_range(x - timedelta(7), x - timedelta(1)).tolist()\n",
    "        #                         for x in dates_eastern]\n",
    "        #                        for date in sublist]\n",
    "        # dates_eastern_p1w = [date for sublist in\n",
    "        #                    [pd.date_range(x + timedelta(1), x + timedelta(7)).tolist()\n",
    "        #                     for x in dates_eastern]\n",
    "        #                    for date in sublist]\n",
    "        # df_holiday_binary.loc[dates_eastern, 'eastern'] = 1\n",
    "        # df_holiday_binary.loc[dates_eastern_m1w, 'easter_m1w'] = 1\n",
    "        # df_holiday_binary.loc[dates_eastern_p1w, 'easter_p1w'] = 1\n",
    "        \n",
    "        dates_ThanksGiving = query_holidays(country, channel, device_type,'Thanksgiving',start_year,end_year)\n",
    "        dates_ThanksGiving_m4w = [date for sublist in\n",
    "                                  [pd.date_range(x - timedelta(28), x - timedelta(22)).tolist() for x in\n",
    "                                   dates_ThanksGiving] for date in sublist]\n",
    "    \n",
    "        dates_ThanksGiving_m3w = [date for sublist in\n",
    "                                  [pd.date_range(x - timedelta(21), x - timedelta(15)).tolist() for x in\n",
    "                                   dates_ThanksGiving] for date in sublist]\n",
    "    \n",
    "        dates_ThanksGiving_m2w = [date for sublist in\n",
    "                                  [pd.date_range(x - timedelta(14), x - timedelta(8)).tolist() for x in\n",
    "                                   dates_ThanksGiving] for date in sublist]\n",
    "    \n",
    "        dates_ThanksGiving_m1w = [date for sublist in\n",
    "                                  [pd.date_range(x - timedelta(7), x - timedelta(2)).tolist() for x in\n",
    "                                   dates_ThanksGiving] for date in sublist]\n",
    "    \n",
    "        df_holiday_binary.loc[dates_ThanksGiving, 'TG'] = 1\n",
    "        df_holiday_binary.loc[dates_ThanksGiving_m3w, 'TG_m3w'] = 1\n",
    "        df_holiday_binary.loc[dates_ThanksGiving_m2w, 'TG_m2w'] = 1\n",
    "        df_holiday_binary.loc[dates_ThanksGiving_m1w, 'TG_m1w'] = 1\n",
    "    \n",
    "        df_holiday_binary['TG_m1d'] = df_holiday_binary['TG'].shift(-1)\n",
    "        df_holiday_binary['TG_p1d'] = df_holiday_binary['TG'].shift(1)\n",
    "        df_holiday_binary['TG_p2d'] = df_holiday_binary['TG'].shift(2)\n",
    "        df_holiday_binary['TG_p3d'] = df_holiday_binary['TG'].shift(3)\n",
    "        df_holiday_binary['TG_p4d'] = df_holiday_binary['TG'].shift(4)\n",
    "    \n",
    "        dates_Xmas = query_holidays(country, channel, device_type, 'Christmas',start_year, end_year)\n",
    "    \n",
    "        dates_Xmas_m3w = [date for sublist in\n",
    "                          [pd.date_range(x - timedelta(21), x - timedelta(15)).tolist()\n",
    "                           for x in dates_Xmas]\n",
    "                          for date in sublist]\n",
    "    \n",
    "        dates_Xmas_m2w = [date for sublist in\n",
    "                          [pd.date_range(x - timedelta(14), x - timedelta(8)).tolist()\n",
    "                           for x in dates_Xmas]\n",
    "                          for date in sublist]\n",
    "    \n",
    "        dates_Xmas_m1w = [date for sublist in\n",
    "                          [pd.date_range(x - timedelta(7), x - timedelta(2)).tolist()\n",
    "                           for x in dates_Xmas]\n",
    "                          for date in sublist]\n",
    "    \n",
    "        dates_Xmas_p1w = [date for sublist in\n",
    "                          [pd.date_range(x + timedelta(1), x + timedelta(7)).tolist()\n",
    "                           for x in dates_Xmas]\n",
    "                          for date in sublist]\n",
    "    \n",
    "        dates_Xmas_p2w = [date for sublist in\n",
    "                          [pd.date_range(x + timedelta(8), x + timedelta(14)).tolist()\n",
    "                           for x in dates_Xmas]\n",
    "                          for date in sublist]\n",
    "    \n",
    "        dates_Xmas_p3w = [date for sublist in\n",
    "                          [pd.date_range(x + timedelta(15), x + timedelta(21)).tolist()\n",
    "                           for x in dates_Xmas]\n",
    "                          for date in sublist]\n",
    "    \n",
    "        dates_Xmas_p4w = [date for sublist in\n",
    "                          [pd.date_range(x + timedelta(22), x + timedelta(28)).tolist()\n",
    "                           for x in dates_Xmas]\n",
    "                          for date in sublist]\n",
    "    \n",
    "        dates_Xmas_p5w = [date for sublist in\n",
    "                          [pd.date_range(x + timedelta(29), x + timedelta(35)).tolist()\n",
    "                           for x in dates_Xmas]\n",
    "                          for date in sublist]\n",
    "    \n",
    "        df_holiday_binary.loc[dates_Xmas, 'CD'] = 1\n",
    "        df_holiday_binary.loc[dates_Xmas_m3w, 'CD_m3w'] = 1\n",
    "        df_holiday_binary.loc[dates_Xmas_m2w, 'CD_m2w'] = 1\n",
    "        df_holiday_binary.loc[dates_Xmas_m1w, 'CD_m1w'] = 1\n",
    "        df_holiday_binary['CDE'] = df_holiday_binary['CD'].shift(-1)\n",
    "        \n",
    "        if country in ['AU','CA','UK']:\n",
    "            df_holiday_binary['BOX'] = df_holiday_binary['CD'].shift(1)\n",
    "        \n",
    "        dates_TG_CD = zip(sorted(dates_ThanksGiving), sorted(dates_Xmas))\n",
    "    \n",
    "        dates_ThanksGiving_p1w = [date for sublist in\n",
    "                                  [pd.date_range(x[0] + timedelta(5),\n",
    "                                                 x[1] - timedelta(22)).tolist()\n",
    "                                   for x in dates_TG_CD]\n",
    "                                  for date in sublist]\n",
    "        df_holiday_binary.loc[dates_ThanksGiving_p1w, 'TG_p1w'] = 1\n",
    "        if country == 'DE':\n",
    "            fathers_day = [dt(2018, 5, 10), dt(2019, 5, 30), dt(2020,5,21), dt(2021, 5, 13)]\n",
    "        else:\n",
    "            fathers_day = query_holidays(country, channel, device_type, 'Father\\'\\'s Day',start_year, end_year)\n",
    "        \n",
    "        dates_father_m1w = [date for sublist in\n",
    "                               [pd.date_range(x - timedelta(7), x - timedelta(1)).tolist()\n",
    "                                for x in fathers_day]\n",
    "                               for date in sublist]\n",
    "        dates_father_p1w = [date for sublist in\n",
    "                           [pd.date_range(x + timedelta(1), x + timedelta(7)).tolist()\n",
    "                            for x in fathers_day]\n",
    "                           for date in sublist]\n",
    "        dates_father_p2w = [date for sublist in\n",
    "                           [pd.date_range(x + timedelta(8), x + timedelta(14)).tolist()\n",
    "                            for x in fathers_day]\n",
    "                           for date in sublist]\n",
    "        df_holiday_binary.loc[fathers_day, 'FD'] = 1\n",
    "        df_holiday_binary.loc[dates_father_m1w, 'FD_m1w'] = 1\n",
    "        df_holiday_binary.loc[dates_father_p1w, 'FD_p1w'] = 1\n",
    "        df_holiday_binary.loc[dates_father_p2w, 'FD_p2w'] = 1\n",
    "        \n",
    "        if country == 'DE':\n",
    "            dates_whitmonday = query_holidays(country, channel, device_type,'Whit Monday',start_year,end_year)\n",
    "            dates_whitmonday_m1w = [date for sublist in\n",
    "                                   [pd.date_range(x - timedelta(7), x - timedelta(1)).tolist()\n",
    "                                    for x in dates_whitmonday]\n",
    "                                   for date in sublist]\n",
    "            dates_whitmonday_p1w = [date for sublist in\n",
    "                               [pd.date_range(x + timedelta(1), x + timedelta(7)).tolist()\n",
    "                                for x in dates_whitmonday]\n",
    "                               for date in sublist]\n",
    "            df_holiday_binary.loc[dates_whitmonday, 'whitmonday'] = 1\n",
    "            df_holiday_binary.loc[dates_whitmonday_m1w, 'whitmonday_m1w'] = 1\n",
    "            df_holiday_binary.loc[dates_whitmonday_p1w, 'whitmonday_p1w'] = 1\n",
    "            \n",
    "            dates_threeking = query_holidays(country, channel, device_type,'Three Kings Day',start_year,end_year)\n",
    "            dates_threeking_m1w = [date for sublist in\n",
    "                                   [pd.date_range(x - timedelta(7), x - timedelta(1)).tolist()\n",
    "                                    for x in dates_threeking]\n",
    "                                   for date in sublist]\n",
    "            dates_threeking_p1w = [date for sublist in\n",
    "                               [pd.date_range(x + timedelta(1), x + timedelta(7)).tolist()\n",
    "                                for x in dates_threeking]\n",
    "                               for date in sublist]\n",
    "            df_holiday_binary.loc[dates_threeking, 'threeking'] = 1\n",
    "            df_holiday_binary.loc[dates_threeking_m1w, 'threeking_m1w'] = 1\n",
    "            df_holiday_binary.loc[dates_threeking_p1w, 'threeking_p1w'] = 1\n",
    "            \n",
    "            dates_holiday = query_holidays(country, channel, device_type,'Labor Day',start_year,end_year)\n",
    "            dates_holiday_m1w = [date for sublist in\n",
    "                                   [pd.date_range(x - timedelta(7), x - timedelta(1)).tolist()\n",
    "                                    for x in dates_holiday]\n",
    "                                   for date in sublist]\n",
    "            dates_holiday_p1w = [date for sublist in\n",
    "                               [pd.date_range(x + timedelta(1), x + timedelta(7)).tolist()\n",
    "                                for x in dates_holiday]\n",
    "                               for date in sublist]\n",
    "            df_holiday_binary.loc[dates_holiday, 'laborday'] = 1\n",
    "            df_holiday_binary.loc[dates_holiday_m1w, 'laborday_m1w'] = 1\n",
    "            df_holiday_binary.loc[dates_holiday_p1w, 'laborday_p1w'] = 1\n",
    "            \n",
    "            dates_holiday = query_holidays(country, channel, device_type,'Labor Day',start_year, end_year)\n",
    "            dates_holiday_m1w = [date for sublist in\n",
    "                                   [pd.date_range(x - timedelta(7), x - timedelta(1)).tolist()\n",
    "                                    for x in dates_holiday]\n",
    "                                   for date in sublist]\n",
    "            dates_holiday_p1w = [date for sublist in\n",
    "                               [pd.date_range(x + timedelta(1), x + timedelta(7)).tolist()\n",
    "                                for x in dates_holiday]\n",
    "                               for date in sublist]\n",
    "            df_holiday_binary.loc[dates_holiday, 'laborday'] = 1\n",
    "            df_holiday_binary.loc[dates_holiday_m1w, 'laborday_m1w'] = 1\n",
    "            df_holiday_binary.loc[dates_holiday_p1w, 'laborday_p1w'] = 1\n",
    "            \n",
    "            dates_holiday = query_holidays(country, channel, device_type,'German Unity Day',start_year, end_year)\n",
    "            dates_holiday_m1w = [date for sublist in\n",
    "                                   [pd.date_range(x - timedelta(7), x - timedelta(1)).tolist()\n",
    "                                    for x in dates_holiday]\n",
    "                                   for date in sublist]\n",
    "            dates_holiday_p1w = [date for sublist in\n",
    "                               [pd.date_range(x + timedelta(1), x + timedelta(7)).tolist()\n",
    "                                for x in dates_holiday]\n",
    "                               for date in sublist]\n",
    "            df_holiday_binary.loc[dates_holiday, 'unityday'] = 1\n",
    "            df_holiday_binary.loc[dates_holiday_m1w, 'unityday_m1w'] = 1\n",
    "            df_holiday_binary.loc[dates_holiday_p1w, 'unityday_p1w'] = 1\n",
    "            \n",
    "            dates_holiday = query_holidays(country, channel, device_type,'Corpus Christi',start_year, end_year)\n",
    "            dates_holiday_m1w = [date for sublist in\n",
    "                                   [pd.date_range(x - timedelta(7), x - timedelta(1)).tolist()\n",
    "                                    for x in dates_holiday]\n",
    "                                   for date in sublist]\n",
    "            dates_holiday_p1w = [date for sublist in\n",
    "                               [pd.date_range(x + timedelta(1), x + timedelta(7)).tolist()\n",
    "                                for x in dates_holiday]\n",
    "                               for date in sublist]\n",
    "            df_holiday_binary.loc[dates_holiday, 'corpuschristi'] = 1\n",
    "            df_holiday_binary.loc[dates_holiday_m1w, 'corpuschristi_m1w'] = 1\n",
    "            df_holiday_binary.loc[dates_holiday_p1w, 'corpuschristi_p1w'] = 1\n",
    "            \n",
    "            dates_holiday = query_holidays(country, channel, device_type,'Assumption Day',start_year, end_year)\n",
    "            dates_holiday_m1w = [date for sublist in\n",
    "                                   [pd.date_range(x - timedelta(7), x - timedelta(1)).tolist()\n",
    "                                    for x in dates_holiday]\n",
    "                                   for date in sublist]\n",
    "            dates_holiday_p1w = [date for sublist in\n",
    "                               [pd.date_range(x + timedelta(1), x + timedelta(7)).tolist()\n",
    "                                for x in dates_holiday]\n",
    "                               for date in sublist]\n",
    "            df_holiday_binary.loc[dates_holiday, 'assumption'] = 1\n",
    "            df_holiday_binary.loc[dates_holiday_m1w, 'assumption_m1w'] = 1\n",
    "            df_holiday_binary.loc[dates_holiday_p1w, 'assumption_p1w'] = 1\n",
    "            \n",
    "            dates_holiday = query_holidays(country, channel, device_type,'Ascension Day',start_year, end_year)\n",
    "            dates_holiday_m1w = [date for sublist in\n",
    "                                   [pd.date_range(x - timedelta(7), x - timedelta(1)).tolist()\n",
    "                                    for x in dates_holiday]\n",
    "                                   for date in sublist]\n",
    "            dates_holiday_p1w = [date for sublist in\n",
    "                               [pd.date_range(x + timedelta(1), x + timedelta(7)).tolist()\n",
    "                                for x in dates_holiday]\n",
    "                               for date in sublist]\n",
    "            df_holiday_binary.loc[dates_holiday, 'accension'] = 1\n",
    "            df_holiday_binary.loc[dates_holiday_m1w, 'accension_m1w'] = 1\n",
    "            df_holiday_binary.loc[dates_holiday_p1w, 'accension_p1w'] = 1\n",
    "            \n",
    "    if (country == 'IN'):\n",
    "        dates_Diwali = query_holidays(country, channel, device_type, 'Diwali',start_year, end_year)\n",
    " \n",
    "        dates_Diwali_p1w = [date for sublist in\n",
    "                            [pd.date_range(x + timedelta(1), x + timedelta(7)).tolist()\n",
    "                             for x in dates_Diwali]\n",
    "                            for date in sublist]\n",
    " \n",
    "        df_holiday_binary.loc[dates_Diwali, 'Diwali'] = 1\n",
    "        df_holiday_binary.loc[dates_Diwali_p1w, 'Diwali_p1w'] = 1\n",
    " \n",
    "        dates_Dussehra = query_holidays(country, channel, device_type, 'Dussehra',start_year, end_year)\n",
    " \n",
    "        dates_Dussehra_p1w = [date for sublist in\n",
    "                              [pd.date_range(x + timedelta(1), x + timedelta(7)).tolist()\n",
    "                               for x in dates_Dussehra]\n",
    "                              for date in sublist]\n",
    " \n",
    "        df_holiday_binary.loc[dates_Dussehra, 'Dussehra'] = 1\n",
    "        df_holiday_binary.loc[dates_Dussehra_p1w, 'Dussehra_p1w'] = 1\n",
    "\n",
    "        dates_Valentine = query_holidays(country, channel, device_type, 'Valentine\\'\\'s Day',start_year, end_year)\n",
    " \n",
    "        dates_Valentine_m1w = [date for sublist in\n",
    "                               [pd.date_range(x - timedelta(7), x - timedelta(1)).tolist()\n",
    "                                for x in dates_Valentine]\n",
    "                               for date in sublist]\n",
    " \n",
    "        df_holiday_binary.loc[dates_Valentine, 'Valentine'] = 1\n",
    "        df_holiday_binary.loc[dates_Valentine_m1w, 'Valentine_m1w'] = 1\n",
    " \n",
    "        dates_Xmas = get_holiday_dates(settings,\n",
    "                                       holiday_name='Christmas')\n",
    " \n",
    "        dates_Xmas_m1w = [date for sublist in\n",
    "                          [pd.date_range(x - timedelta(7), x - timedelta(1)).tolist()\n",
    "                           for x in dates_Xmas]\n",
    "                          for date in sublist]\n",
    " \n",
    "        df_holiday_binary.loc[dates_Xmas, 'CD'] = 1\n",
    "        df_holiday_binary.loc[dates_Xmas_m1w, 'CD_m1w'] = 1\n",
    "    \n",
    "    df_holiday_binary = df_holiday_binary.reset_index(drop=True)\n",
    "    df_holiday_binary = df_holiday_binary.fillna(0)\n",
    "    return df_holiday_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to create seasonality, trend, lag features\n",
    "def create_seasonality_features(period = 'yearly', n_range = None , dates = None): \n",
    "\n",
    "    delta_days = [(x - pd.Timestamp(x.year, 1, 1)).days for x in dates] # number of days since Janunary 1st\n",
    "    if period == 'yearly': \n",
    "        P = 365.25\n",
    "    elif period == 'monthly': \n",
    "        P = 30\n",
    "    elif period == 'weekly': \n",
    "        P = 7\n",
    "    sine = np.array([[np.sin(2.0*i*np.pi*n/P) for i in delta_days] for n in n_range]).T\n",
    "    col_sine_names = ['_'.join(['seasonal', period , 'fourier', 'sine', str(n)]) for n in n_range]\n",
    "    cosine = np.array([[np.cos(2.0*i*np.pi*n/P) for i in delta_days] for n in n_range]).T\n",
    "    col_cosine_names = ['_'.join(['seasonal', period , 'fourier', 'cosine', str(n)]) for n in n_range]\n",
    "    df_sine = pd.DataFrame(sine, columns = col_sine_names)\n",
    "    df_cosine = pd.DataFrame(cosine, columns = col_cosine_names)\n",
    "    df_sine['date'] = dates\n",
    "    df_cosine['date'] = dates\n",
    "    \n",
    "    return {'sine':df_sine, 'cosine':df_cosine}\n",
    "\n",
    "\n",
    "def create_trend_features(df, period = 'semi', trend_start_year = 2017):\n",
    "    \"\"\"\n",
    "        create trend features based on period\n",
    "    \"\"\" \n",
    "\n",
    "    dates = df['date']\n",
    "\n",
    "    if period == 'annual': \n",
    "        n_days = int(365)\n",
    "    elif period == 'semi-annual': \n",
    "        n_days = int(365/2)\n",
    "    elif period == 'quarterly': \n",
    "        n_days = int(365/4) + 1\n",
    "    elif period == 'monthly': \n",
    "        n_days = 30\n",
    "    else: \n",
    "        raise('Choose period from [annual, semi-annual, quarterly, monthly]')\n",
    "\n",
    "    trend_start_date = pd.Timestamp(trend_start_year,1,1) # trend start date \n",
    "    current_date = pd.to_datetime('today').date() \n",
    "    \n",
    "    trend_referenced_dates = pd.date_range(trend_start_date, current_date, freq = '{}D'.format(n_days)) # generate referenced dates based on freq\n",
    "    delta_days = np.array([[(x - trend_referenced_date).days for x in dates] for trend_referenced_date in trend_referenced_dates]) \n",
    "    df_trend = pd.DataFrame(delta_days.T, columns = ['_'.join([period, 'trend_since', str(x.year), str(x.month)]) for x in trend_referenced_dates])\n",
    "    df_trend[df_trend < 0] = 0 # set all negative values to 0 \n",
    "    df_trend = np.log(df_trend + 1.0) # log-transform\n",
    "    df_trend['date'] = dates\n",
    "    df_trend['asin'] = df.asin\n",
    "\n",
    "    return df_trend\n",
    "\n",
    "def generate_lag_lead_features(df, lags):\n",
    "    \n",
    "    # helper function\n",
    "    def lag_by_group(key, value_df, lag = 1):\n",
    "        df = value_df.assign(group = key) # this pandas method returns a copy of the df, with group columns assigned the key value\n",
    "        return (df.sort_values(by=[\"date\"], ascending=True)\n",
    "            .set_index([\"date\"])\n",
    "            .shift(lag))\n",
    "    \n",
    "    temp = df\n",
    "    for lag in lags: \n",
    "        grouped_df = df[['asp', 'vlt', 'asin', 'date']].groupby([\"asin\"])\n",
    "        dflist = [lag_by_group(g, grouped_df.get_group(g), lag) for g in grouped_df.groups.keys()]\n",
    "        result_lags = pd.concat(dflist, axis=0).reset_index()\n",
    "        if lag > 0: \n",
    "            columns = ['date', '_'.join(['asp','lag', str(lag)]), '_'.join(['vlt','lag', str(lag)]) , 'asin']\n",
    "        if lag < 0: \n",
    "            columns = ['date', '_'.join(['asp','lead', str(-lag)]), '_'.join(['vlt','lead', str(-lag)]) , 'asin']\n",
    "        result_lags = result_lags.drop('group', axis = 1)\n",
    "        result_lags.columns = columns\n",
    "        temp = pd.merge(temp, result_lags, on = ['date', 'asin'], how = 'left')\n",
    "\n",
    "    temp = temp.drop(['asp', 'vlt'], axis = 1)\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pre_process(country, channel, device, start_date, end_date):\n",
    "    \n",
    "\n",
    "    df_vlt = query_vlt(country, channel, device, start_date, end_date)\n",
    "    df_asp = query_asp(country, channel, device, start_date, end_date)\n",
    "    df_actuals = query_actuals(country, channel, device, pd.to_datetime(start_date), pd.to_datetime(end_date))\n",
    "    \n",
    "    df_age = query_asin_end(country, channel, device)\n",
    "    df_age['life_value_street'] = (df_age['asin_end_date'] - df_age['asin_start_date']).dt.days\n",
    "    df_age['log_age'] = np.log(df_age['life_value_street'])\n",
    "    df_all = pd.merge(df_asp, df_age, on = ['asin', 'program'], how = 'left')\n",
    "    df_all['age'] = np.log((df_all['date'] - df_all['asin_start_date']).dt.days + 1.0)\n",
    "\n",
    "    df_asin_attr = query_asin_attr(country, channel, device, start_date, end_date)\n",
    "    df_asin_attr = df_asin_attr.drop_duplicates(subset = 'asin') # drop duplicates\n",
    "    df_all = pd.merge(df_all, df_asin_attr[['asin', 'storage', 'bundle', 'dtcp', 'ket_color', 'msrp', 'color_name', 'sub_category']], on = ['asin'], how = 'left')\n",
    "    df_all = pd.merge(df_all, df_vlt, on = ['date', 'asin'], how = 'left')\n",
    "    # process nan's \n",
    "    df_all.loc[df_all.alp.isna(), 'alp'] = df_all.loc[df_all.alp.isna(), 'msrp'] # replace alp nans by msrp\n",
    "    df_all.loc[df_all.asp.isna(), 'asp'] = df_all.loc[df_all.asp.isna(), 'alp'] # replace asp nans by alp \n",
    "    df_all['asp'] = np.log(df_all['asp']) # take log of asp and call asp for simplicity\n",
    "    df_all['discount'] = (df_all['alp'] - df_all['asp'])/df_all['alp']\n",
    "    df_all.loc[df_all.discount.isna(), 'discount'] = 0.0  # set discount nan's to 0\n",
    "    df_all.loc[df_all.vlt > 365, 'vlt'] = 365\n",
    "    df_all['vlt'] = np.log(df_all['vlt'] + 1.0)\n",
    "    df_all.loc[df_all.vlt.isna(), 'vlt'] = 0.0\n",
    "    df_all = pd.merge(df_all, df_actuals[['date', 'asin', 'actual']], on = ['date', 'asin'], how = 'left')\n",
    "    \n",
    "    # generate lead and lag variables\n",
    "    df_lags = generate_lag_lead_features(df_all[['asp', 'vlt', 'asin', 'date']], lags = range(1,7)) # 7 lags\n",
    "    df_leads = generate_lag_lead_features(df_all[['asp', 'vlt', 'asin', 'date']], lags = range(-7,0)) # 7 leads\n",
    "    temp = pd.merge(df_lags, df_leads, on = ['asin', 'date'])\n",
    "\n",
    "    dict_seasonality = create_seasonality_features(period = 'yearly', n_range = range(1,21) , dates = pd.to_datetime(df_asp.date.unique()))\n",
    "    temp = pd.merge(temp, dict_seasonality['sine'], on = 'date', how = 'left')\n",
    "    temp = pd.merge(temp, dict_seasonality['cosine'], on = 'date', how = 'left')\n",
    "\n",
    "    # create annual trend\n",
    "    for period in ['annual']: \n",
    "        df_trend = create_trend_features(period = period, df = temp[['date','asin']])\n",
    "        temp = pd.merge(temp, df_trend, on = ['date', 'asin'])\n",
    "\n",
    "    df_all = pd.merge(df_all, temp, on = ['date', 'asin'], how = 'left')\n",
    "\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_one_hot_encoding(df, drop_first = False):\n",
    "    unique_values = df.apply(lambda x: len(x.unique())).values\n",
    "    cat_vars = df.columns[(unique_values < 40) & (unique_values > 2)]\n",
    "    for var in cat_vars:\n",
    "        if var in df.columns:\n",
    "            if var not in ['asin', 'covid', 'log_age', 'vlt', 'Customer-Facing Name', 'log_asp', 'log_alp', 'program']:\n",
    "                df = df.join(pd.get_dummies(df[var],prefix = var, drop_first = drop_first))\n",
    "                if var != 'program':\n",
    "                    df = df.drop(var, axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df_input(country, channel, device, start_date, end_date):\n",
    "\n",
    "    df = pre_process(country, channel, device, start_date, end_date)\n",
    "    df_holidays = generate_holidays(country, channel, device, start_date, end_date)\n",
    "    df_category = pd.read_csv('program_category.csv') # will fix this for Eider note book \n",
    "    df = pd.merge(df, df_category, on = 'program', how = 'left')\n",
    "    df_holidays = generate_holidays(country, channel, device, start_date, end_date)\n",
    "    df = pd.merge(df, df_holidays, on = 'date', how = 'left')\n",
    "    names = df['Customer-Facing Name']\n",
    "    df['is_dot'] =  np.array(['Dot' in x for x in names], dtype = 'int')\n",
    "    df['is_show'] = np.array(['Show' in x for x in names], dtype = 'int')\n",
    "    df['is_plus'] = np.array(['Plus' in x for x in names], dtype = 'int')\n",
    "    df['covid'] = df.date >= '2020-03-15'\n",
    "    df['weekday'] = np.array(df['date'].apply(lambda x: x.weekday()))\n",
    "    df['month'] = np.array(df['date'].apply(lambda x: x.month))\n",
    "\n",
    "    return df\n",
    "\n",
    "# df_temp = convert_one_hot_encoding(df)\n",
    "# df_temp = df_temp[(~df_temp.actual.isna()) & (df_temp.date <= END_DATE)]\n",
    "# return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backcast(df, BACKCAST_START_DATE, BACKCAST_END_DATE, horizon = 24):\n",
    "\n",
    "\n",
    "    drop_vars = ['channel', 'asin', 'log_actual', 'baseline', 'actual_last', 'msrp', \n",
    "                'nrows', 'country', 'channel', 'vdps', 'wireleconvert_one_hot_encodingsqrt_actual',  'age', \n",
    "            'Customer-Facing Name', 'qty', 'ket_color', 'life_value_street', 'asin_end_date', 'Category', 'bundle', 'dtcp', 'device_type', 'alp', 'asin_start_date']\n",
    "    for var in drop_vars: \n",
    "        if var in df.columns: \n",
    "            df = df.drop(var, axis = 1)\n",
    "\n",
    "    \n",
    "    df_temp = convert_one_hot_encoding(df)\n",
    "    df_temp = df_temp[(~df_temp.actual.isna()) & (df_temp.date <= END_DATE)]\n",
    "\n",
    "    remove_vars = df_temp.columns[['vlt_lead' in x for x in df_temp.columns]]\n",
    "    df_temp = df_temp.drop(remove_vars, 1)\n",
    "\n",
    "\n",
    "#     remove_vars = df_temp.columns[['asp_lag' in x for x in df_temp.columns]]\n",
    "#     df_temp = df_temp.drop(remove_vars, 1)\n",
    "\n",
    "#     remove_vars = df_temp.columns[['annual_trend' in x for x in df_temp.columns]]\n",
    "#     df_temp = df_temp.drop(remove_vars, 1)\n",
    "\n",
    "#     remove_vars = df_temp.columns[['quarterly_trend' in x for x in df_temp.columns]]\n",
    "#     df_temp = df_temp.drop(remove_vars, 1)\n",
    "\n",
    "\n",
    "    BACKCAST_PERIODS = pd.date_range(start = BACKCAST_START_DATE, end = BACKCAST_END_DATE , freq = 'W-MON')\n",
    "\n",
    "    ridge_model = CCARD_MODEL(feature_selection = True, hyperparameter_tuning = False,  random_state = 0, model_name = 'ridge', transformation = 'log')\n",
    "    df_horizon, df_result_all = ridge_model.back_cast(df_temp, BACKCAST_PERIODS, BACKCAST_END_DATE, horizon, retrain = True, metric_clip = True)\n",
    "\n",
    "    return df_horizon.sort_values('program')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_forecast(df,  training_start_date, training_end_date, forecast_end_date):\n",
    "    \n",
    "    forecast_end_date = min(pd.to_datetime(training_end_date) + timedelta(30*7), pd.to_datetime(forecast_end_date))\n",
    "\n",
    "    drop_vars = ['channel', 'asin', 'log_actual', 'baseline', 'actual_last', 'msrp', \n",
    "                'nrows',  'vdps', 'wireless_type', 'sqrt_actual',  'age', \n",
    "            'Customer-Facing Name', 'qty', 'ket_color', 'life_value_street', 'asin_end_date', 'Category', 'bundle', 'dtcp',  'alp', 'asin_start_date']\n",
    "\n",
    "    df_final_forecast = df.query('date >= @training_end_date & date < @forecast_end_date')[['date','country','channel','device_type','program','asin','actual']]\n",
    "\n",
    "\n",
    "    for var in drop_vars: \n",
    "        if var in df.columns: \n",
    "            df = df.drop(var, axis = 1)\n",
    "\n",
    "    df = convert_one_hot_encoding(df)\n",
    "    df_train = df.query('date < @training_end_date')\n",
    "\n",
    "    df_train = df_train[~df_train.actual.isna()]\n",
    "    ridge_model = CCARD_MODEL(feature_selection = True, hyperparameter_tuning = True,  random_state = 0, model_name = 'ridge', transformation = 'log')\n",
    "    ridge_model.fit(df_train.drop('actual', axis = 1), np.log(df_train['actual'] + 0.0001))\n",
    "\n",
    "    df_pred = df.query('date >= @training_end_date & date < @forecast_end_date')\n",
    "    y_pred = ridge_model.predict(df_pred[ridge_model.selected_cols])\n",
    "    df_final_forecast['vdp_plus_units'] = y_pred\n",
    "\n",
    "    return df_final_forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 s, sys: 0 ns, total: 11 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from LR_prod import *\n",
    "COUNTRY = 'US'  \n",
    "CHANNEL='ONLINE'\n",
    "DEVICE = 'AuCC'\n",
    "START_DATE= '2016-05-01'\n",
    "END_DATE='2020-12-31'\n",
    "BACKCAST_START_DATE = '2019-11-04'\n",
    "BACKCAST_END_DATE = '2020-05-25'\n",
    "COVID_START = '2020-03-15' # assume covid start at the beginning of march\n",
    "\n",
    "training_end_date = '2020-06-24'\n",
    "forecast_end_date = '2020-12-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_df_input(COUNTRY, CHANNEL, DEVICE, START_DATE, END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Preparing training and test sets ...\n",
      "Training model up to 2019-11-04 and forecasting up to 2020-04-20\n",
      "Running feature selection ...\n",
      "Dropping constant vars: ['covid']\n",
      "The best param(s) are {'alpha': 0.001}\n",
      "Fold 2\n",
      "Preparing training and test sets ...\n",
      "Training model up to 2019-11-11 and forecasting up to 2020-04-27\n",
      "Running feature selection ...\n",
      "Dropping constant vars: ['covid']\n",
      "The best param(s) are {'alpha': 0.001}\n",
      "Fold 3\n",
      "Preparing training and test sets ...\n",
      "Training model up to 2019-11-18 and forecasting up to 2020-05-04\n",
      "Running feature selection ...\n",
      "Dropping constant vars: ['covid']\n",
      "The best param(s) are {'alpha': 0.001}\n",
      "Fold 4\n",
      "Preparing training and test sets ...\n",
      "Training model up to 2019-11-25 and forecasting up to 2020-05-11\n",
      "Running feature selection ...\n",
      "Dropping constant vars: ['covid']\n",
      "The best param(s) are {'alpha': 0.001}\n",
      "Fold 5\n",
      "Preparing training and test sets ...\n",
      "Training model up to 2019-12-02 and forecasting up to 2020-05-18\n",
      "Running feature selection ...\n",
      "Dropping constant vars: ['covid']\n",
      "The best param(s) are {'alpha': 0.001}\n",
      "Fold 6\n",
      "Preparing training and test sets ...\n",
      "Training model up to 2019-12-09 and forecasting up to 2020-05-25\n",
      "Running feature selection ...\n",
      "Dropping constant vars: ['covid']\n",
      "The best param(s) are {'alpha': 0.001}\n",
      "Fold 7\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 8\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 9\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 10\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 11\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 12\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 13\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 14\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 15\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 16\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 17\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 18\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 19\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 20\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 21\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 22\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 23\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 24\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 25\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 26\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 27\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 28\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 29\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "Fold 30\n",
      "Preparing training and test sets ...\n",
      "Less than 24 weeks to forecast\n",
      "      program     wmape     wbias      weight  horizon\n",
      "17       AuCC  0.236874 -0.046746         NaN       24\n",
      "0     Biscuit  1.000000  1.000000     19340.0       24\n",
      "1      Bishop  0.168491 -0.152437    677918.0       24\n",
      "2    Checkers  0.204818  0.042326   6796423.0       24\n",
      "3   Croissant  0.471647 -0.471647    949163.0       24\n",
      "4       Crown  0.496897 -0.496897   3269773.0       24\n",
      "5     Crumpet  0.154098  0.022567  29321946.0       24\n",
      "6     Cupcake  0.255333 -0.239225    584905.0       24\n",
      "7    Doebrite  0.713934 -0.713934   1681856.0       24\n",
      "8       Donut  0.358339  0.358339     17717.0       24\n",
      "9       Lidar  0.139389 -0.139389    452189.0       24\n",
      "10     Muffin  0.579797 -0.082716   2109801.0       24\n",
      "11     Octave  0.762835 -0.762835    225642.0       24\n",
      "12     Pascal  0.112446  0.095322   2579420.0       24\n",
      "13      Puget  1.000000  1.000000    375587.0       24\n",
      "14      Radar  1.000000  1.000000     24664.0       24\n",
      "15       Rook  0.531507  0.487020     19182.0       24\n",
      "16      Sonar  1.000000  1.000000        68.0       24\n"
     ]
    }
   ],
   "source": [
    "backcast_result = run_backcast(df, BACKCAST_START_DATE, BACKCAST_END_DATE, horizon = 24)\n",
    "print(backcast_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running feature selection ...\n",
      "Running hyperparameter tuning ...\n"
     ]
    }
   ],
   "source": [
    "# generate future forecasts\n",
    "df_final_forecast = generate_forecast(df, START_DATE, training_end_date, forecast_end_date)\n",
    "print(df_final_forecast[df_final_forecast['actual'] > 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running feature selection ...\n",
      "Running hyperparameter tuning ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3280839.0, 2391755.8152228035)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check in sample Covid period\n",
    "df_final_forecast = generate_forecast(df, START_DATE, '2020-03-15', '2020-07-18')\n",
    "df_final_forecast['actual'].sum(), df_final_forecast['vdp_plus_units'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>channel</th>\n",
       "      <th>device_type</th>\n",
       "      <th>program</th>\n",
       "      <th>asin</th>\n",
       "      <th>actual</th>\n",
       "      <th>vdp_plus_units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2877</th>\n",
       "      <td>2020-06-24</td>\n",
       "      <td>US</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>AuCC</td>\n",
       "      <td>Crumpet</td>\n",
       "      <td>B07PX3SCFN</td>\n",
       "      <td>159.0</td>\n",
       "      <td>121.569439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2878</th>\n",
       "      <td>2020-06-25</td>\n",
       "      <td>US</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>AuCC</td>\n",
       "      <td>Crumpet</td>\n",
       "      <td>B07PX3SCFN</td>\n",
       "      <td>155.0</td>\n",
       "      <td>122.868041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>2020-06-26</td>\n",
       "      <td>US</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>AuCC</td>\n",
       "      <td>Crumpet</td>\n",
       "      <td>B07PX3SCFN</td>\n",
       "      <td>146.0</td>\n",
       "      <td>120.640974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>2020-06-27</td>\n",
       "      <td>US</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>AuCC</td>\n",
       "      <td>Crumpet</td>\n",
       "      <td>B07PX3SCFN</td>\n",
       "      <td>179.0</td>\n",
       "      <td>118.115260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2881</th>\n",
       "      <td>2020-06-28</td>\n",
       "      <td>US</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>AuCC</td>\n",
       "      <td>Crumpet</td>\n",
       "      <td>B07PX3SCFN</td>\n",
       "      <td>192.0</td>\n",
       "      <td>123.423165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31762</th>\n",
       "      <td>2020-07-14</td>\n",
       "      <td>US</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>AuCC</td>\n",
       "      <td>Lidar</td>\n",
       "      <td>B07CT3W44K</td>\n",
       "      <td>98.0</td>\n",
       "      <td>41.749391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31763</th>\n",
       "      <td>2020-07-15</td>\n",
       "      <td>US</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>AuCC</td>\n",
       "      <td>Lidar</td>\n",
       "      <td>B07CT3W44K</td>\n",
       "      <td>123.0</td>\n",
       "      <td>39.143750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31764</th>\n",
       "      <td>2020-07-16</td>\n",
       "      <td>US</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>AuCC</td>\n",
       "      <td>Lidar</td>\n",
       "      <td>B07CT3W44K</td>\n",
       "      <td>305.0</td>\n",
       "      <td>36.148382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31765</th>\n",
       "      <td>2020-07-17</td>\n",
       "      <td>US</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>AuCC</td>\n",
       "      <td>Lidar</td>\n",
       "      <td>B07CT3W44K</td>\n",
       "      <td>171.0</td>\n",
       "      <td>23.009991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31766</th>\n",
       "      <td>2020-07-19</td>\n",
       "      <td>US</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>AuCC</td>\n",
       "      <td>Lidar</td>\n",
       "      <td>B07CT3W44K</td>\n",
       "      <td>29.0</td>\n",
       "      <td>8.527970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>718 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date country channel device_type  program        asin  actual  \\\n",
       "2877  2020-06-24      US  ONLINE        AuCC  Crumpet  B07PX3SCFN   159.0   \n",
       "2878  2020-06-25      US  ONLINE        AuCC  Crumpet  B07PX3SCFN   155.0   \n",
       "2879  2020-06-26      US  ONLINE        AuCC  Crumpet  B07PX3SCFN   146.0   \n",
       "2880  2020-06-27      US  ONLINE        AuCC  Crumpet  B07PX3SCFN   179.0   \n",
       "2881  2020-06-28      US  ONLINE        AuCC  Crumpet  B07PX3SCFN   192.0   \n",
       "...          ...     ...     ...         ...      ...         ...     ...   \n",
       "31762 2020-07-14      US  ONLINE        AuCC    Lidar  B07CT3W44K    98.0   \n",
       "31763 2020-07-15      US  ONLINE        AuCC    Lidar  B07CT3W44K   123.0   \n",
       "31764 2020-07-16      US  ONLINE        AuCC    Lidar  B07CT3W44K   305.0   \n",
       "31765 2020-07-17      US  ONLINE        AuCC    Lidar  B07CT3W44K   171.0   \n",
       "31766 2020-07-19      US  ONLINE        AuCC    Lidar  B07CT3W44K    29.0   \n",
       "\n",
       "       vdp_plus_units  \n",
       "2877       121.569439  \n",
       "2878       122.868041  \n",
       "2879       120.640974  \n",
       "2880       118.115260  \n",
       "2881       123.423165  \n",
       "...               ...  \n",
       "31762       41.749391  \n",
       "31763       39.143750  \n",
       "31764       36.148382  \n",
       "31765       23.009991  \n",
       "31766        8.527970  \n",
       "\n",
       "[718 rows x 8 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VDP",
   "language": "python",
   "name": "dlwp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
